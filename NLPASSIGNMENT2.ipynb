{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ce68e295a6bf434889adeeb3bb38f051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56d3122950964c569417b7eb87378877",
              "IPY_MODEL_4ee7815b968d4e0c8a83473655f77b15",
              "IPY_MODEL_1175dead08b14e47b6b9ac9865f2b710"
            ],
            "layout": "IPY_MODEL_1b8bfa3109a741b9854e61a57d075299"
          }
        },
        "56d3122950964c569417b7eb87378877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dbc3346e69b4e3c90523c569e37a077",
            "placeholder": "​",
            "style": "IPY_MODEL_8e1dd402040a4815b7192d016b443ae1",
            "value": "Dl Completed...: 100%"
          }
        },
        "4ee7815b968d4e0c8a83473655f77b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bef9f3eb56bd456792a7c8bf81acff91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_281b053b3550494481e31704b8acb7cd",
            "value": 1
          }
        },
        "1175dead08b14e47b6b9ac9865f2b710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e6d3b10bdba442b96efa303afc07f98",
            "placeholder": "​",
            "style": "IPY_MODEL_f7272c7f4239427f87a6d1be96c38006",
            "value": " 1/1 [00:03&lt;00:00,  3.43s/ url]"
          }
        },
        "1b8bfa3109a741b9854e61a57d075299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dbc3346e69b4e3c90523c569e37a077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1dd402040a4815b7192d016b443ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bef9f3eb56bd456792a7c8bf81acff91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "281b053b3550494481e31704b8acb7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e6d3b10bdba442b96efa303afc07f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7272c7f4239427f87a6d1be96c38006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a7cb7c8ce4a454dbc14c23c520e706d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d0ae9efb1c94c1daf1c1a95f7faa348",
              "IPY_MODEL_d051df38613048838b3186d1d6aa1827",
              "IPY_MODEL_bd29ea47cbc64453965d905ba85067e5"
            ],
            "layout": "IPY_MODEL_e0d9212f014d4ff795e90772b11f8fd1"
          }
        },
        "5d0ae9efb1c94c1daf1c1a95f7faa348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29097fad2fc84c89b8f18f575b51a44b",
            "placeholder": "​",
            "style": "IPY_MODEL_991bc0421328428785bacd9aafb62eea",
            "value": "Dl Size...: 100%"
          }
        },
        "d051df38613048838b3186d1d6aa1827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b064ba3549454318af9f2abd4527392b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8801ff9cf5504eedaef116c4f20bfb57",
            "value": 1
          }
        },
        "bd29ea47cbc64453965d905ba85067e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72780f86f1c947b6b23cb765989f0c76",
            "placeholder": "​",
            "style": "IPY_MODEL_8e84c7a503314ddebb75d2509065e4fa",
            "value": " 80/80 [00:03&lt;00:00, 30.08 MiB/s]"
          }
        },
        "e0d9212f014d4ff795e90772b11f8fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29097fad2fc84c89b8f18f575b51a44b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991bc0421328428785bacd9aafb62eea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b064ba3549454318af9f2abd4527392b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8801ff9cf5504eedaef116c4f20bfb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72780f86f1c947b6b23cb765989f0c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e84c7a503314ddebb75d2509065e4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82b77af544cd44968c63d897bbadcc6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5194eed43934e458535031c8f61f23b",
              "IPY_MODEL_b865bbd0e30d4e6abf7d73c589d984c5",
              "IPY_MODEL_ff5a29aa74964720ac68b51ed10f03fd"
            ],
            "layout": "IPY_MODEL_8700f1f80b87457e91c52f5bfa9129b0"
          }
        },
        "c5194eed43934e458535031c8f61f23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_444f4206d933448891ebac5cd0096720",
            "placeholder": "​",
            "style": "IPY_MODEL_1433dc6d3e934e5484a3659e70420fb6",
            "value": "Generating splits...: 100%"
          }
        },
        "b865bbd0e30d4e6abf7d73c589d984c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83dd7c00e0974fb4b784dbd8fba83cbc",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_346b4a40e5c34d9aa723c880e9a08f26",
            "value": 3
          }
        },
        "ff5a29aa74964720ac68b51ed10f03fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3904d121c184b5385d1b264b23c1811",
            "placeholder": "​",
            "style": "IPY_MODEL_414aa29cdf5d473aa6d44a71fe1d501c",
            "value": " 3/3 [00:29&lt;00:00, 10.03s/ splits]"
          }
        },
        "8700f1f80b87457e91c52f5bfa9129b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "444f4206d933448891ebac5cd0096720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1433dc6d3e934e5484a3659e70420fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83dd7c00e0974fb4b784dbd8fba83cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346b4a40e5c34d9aa723c880e9a08f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3904d121c184b5385d1b264b23c1811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "414aa29cdf5d473aa6d44a71fe1d501c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da4cf1ca0542433a9345aa833941304a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bab4e18e8ed142e78b9232730cd83e93",
              "IPY_MODEL_fd4bf250b8ce4e45bcad10864021f7f5",
              "IPY_MODEL_78ba1b4de00b4b24b05fe3940bf4268d"
            ],
            "layout": "IPY_MODEL_36f19a3b8f984c4bb1b04bfbda348314"
          }
        },
        "bab4e18e8ed142e78b9232730cd83e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_753de67d93e947d291e9512f2959c4ef",
            "placeholder": "​",
            "style": "IPY_MODEL_6f38ac500c35473bbe25c8640b72047a",
            "value": "Generating train examples...:  91%"
          }
        },
        "fd4bf250b8ce4e45bcad10864021f7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b64b2129068401db13089485b7e9ba6",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d3e43d85a954839af46373e47f100a6",
            "value": 25000
          }
        },
        "78ba1b4de00b4b24b05fe3940bf4268d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c257a7a3062a411ca17589cb95789a4e",
            "placeholder": "​",
            "style": "IPY_MODEL_09e4d74310794fc78911ebdce58e07e4",
            "value": " 22843/25000 [00:05&lt;00:00, 5317.44 examples/s]"
          }
        },
        "36f19a3b8f984c4bb1b04bfbda348314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "753de67d93e947d291e9512f2959c4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f38ac500c35473bbe25c8640b72047a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b64b2129068401db13089485b7e9ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d3e43d85a954839af46373e47f100a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c257a7a3062a411ca17589cb95789a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e4d74310794fc78911ebdce58e07e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "244b38d54dbb48efa91f69852dd369c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_775b274eac994f7cba48a52a4a7fa9ce",
              "IPY_MODEL_b3ae3738e77948c986f666706b890e3d",
              "IPY_MODEL_97d02a9457674cf5b599433f4f61db9b"
            ],
            "layout": "IPY_MODEL_74454e3ecadd4fdf873a2a9f06a64aec"
          }
        },
        "775b274eac994f7cba48a52a4a7fa9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44c5bc7a2f23413ca79d131f72b4345d",
            "placeholder": "​",
            "style": "IPY_MODEL_417f1f2c349d49919dcc7bf025a6aef5",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.6JTSFB_1.0.0/imdb_reviews-train.tfrecord*...:   0%"
          }
        },
        "b3ae3738e77948c986f666706b890e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73c4a7e16df3474e983a0369b5ff4bb6",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4c612c8851c4f459e987eba6674c7b2",
            "value": 25000
          }
        },
        "97d02a9457674cf5b599433f4f61db9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c71d356016e340eb8ef736e635bd2746",
            "placeholder": "​",
            "style": "IPY_MODEL_a05ac2525ddf401da14ea447592e71be",
            "value": " 0/25000 [00:00&lt;?, ? examples/s]"
          }
        },
        "74454e3ecadd4fdf873a2a9f06a64aec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "44c5bc7a2f23413ca79d131f72b4345d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "417f1f2c349d49919dcc7bf025a6aef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73c4a7e16df3474e983a0369b5ff4bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c612c8851c4f459e987eba6674c7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c71d356016e340eb8ef736e635bd2746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05ac2525ddf401da14ea447592e71be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1948c44dcb02452d9348598966f5fa26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1c5627363de4f9d94fe9e50e523d7a2",
              "IPY_MODEL_4e6fe70677be4780a4aa1c796a90a9dd",
              "IPY_MODEL_c34a53c4bc604cb6a743b206e469fb72"
            ],
            "layout": "IPY_MODEL_8da05ce15d4441f4addfb48fa009221b"
          }
        },
        "d1c5627363de4f9d94fe9e50e523d7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e033c39bdee42d18f1c2f8dd60c43bf",
            "placeholder": "​",
            "style": "IPY_MODEL_28d3086ee10141edb1ccd3b026cd1bf9",
            "value": "Generating test examples...:  76%"
          }
        },
        "4e6fe70677be4780a4aa1c796a90a9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f51154fd8154ad49f8891cf7f818a40",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bc80d615a1d4f05ac6142591f5bd1d0",
            "value": 25000
          }
        },
        "c34a53c4bc604cb6a743b206e469fb72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50616f5d2fb34aef8000a952108a2937",
            "placeholder": "​",
            "style": "IPY_MODEL_c074447f2d4746da843ed76507e80bd4",
            "value": " 18959/25000 [00:03&lt;00:00, 6750.36 examples/s]"
          }
        },
        "8da05ce15d4441f4addfb48fa009221b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "0e033c39bdee42d18f1c2f8dd60c43bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28d3086ee10141edb1ccd3b026cd1bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f51154fd8154ad49f8891cf7f818a40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc80d615a1d4f05ac6142591f5bd1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50616f5d2fb34aef8000a952108a2937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c074447f2d4746da843ed76507e80bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09e309f68b364eb98a445a03cd437892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdd65a3833fc439fa6b55013242fe88e",
              "IPY_MODEL_a0ab907dd6ad44ecbf032ab54e59a56d",
              "IPY_MODEL_6de95d16796342179990dea48c3210d8"
            ],
            "layout": "IPY_MODEL_6ae39e67d4484079aca6184ce375bbff"
          }
        },
        "fdd65a3833fc439fa6b55013242fe88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec6fe0577264791b906d25768d54f7a",
            "placeholder": "​",
            "style": "IPY_MODEL_8cd84be806e64491bece277230343ccf",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.6JTSFB_1.0.0/imdb_reviews-test.tfrecord*...:   0%"
          }
        },
        "a0ab907dd6ad44ecbf032ab54e59a56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_819b23a743c4422bbc8c6967452ff5dd",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8424fa64043043a692632e2919e7af53",
            "value": 25000
          }
        },
        "6de95d16796342179990dea48c3210d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44141aa1ab004b1dba55c5039de06d43",
            "placeholder": "​",
            "style": "IPY_MODEL_4aa1005c6f1142fab2babfcf5d21b386",
            "value": " 0/25000 [00:00&lt;?, ? examples/s]"
          }
        },
        "6ae39e67d4484079aca6184ce375bbff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "7ec6fe0577264791b906d25768d54f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd84be806e64491bece277230343ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "819b23a743c4422bbc8c6967452ff5dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8424fa64043043a692632e2919e7af53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44141aa1ab004b1dba55c5039de06d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa1005c6f1142fab2babfcf5d21b386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "401b0d18e34f4d1aa1c7c99f59fbb236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0bbba0f6cbb4e079c78daaf569b30ec",
              "IPY_MODEL_9a2180a3d0fb46879955c6bdd4edf2dc",
              "IPY_MODEL_fb50f678afc64849b045297314a74909"
            ],
            "layout": "IPY_MODEL_14603c7012064cd0af76b6bf08466503"
          }
        },
        "a0bbba0f6cbb4e079c78daaf569b30ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_298884d6a9c849d191a033570fd31d27",
            "placeholder": "​",
            "style": "IPY_MODEL_7da30fbd54e94a1986b202b0c23c6063",
            "value": "Generating unsupervised examples...:  99%"
          }
        },
        "9a2180a3d0fb46879955c6bdd4edf2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59363ae7d1b04a2096422a604565aac5",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52e5246c37654fc1a7ec18c1a7cb22de",
            "value": 50000
          }
        },
        "fb50f678afc64849b045297314a74909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87831eb8ee694cddb843f57808e26858",
            "placeholder": "​",
            "style": "IPY_MODEL_1e3fed6359b64794a28078a995eb8f07",
            "value": " 49396/50000 [00:10&lt;00:00, 5992.43 examples/s]"
          }
        },
        "14603c7012064cd0af76b6bf08466503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "298884d6a9c849d191a033570fd31d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7da30fbd54e94a1986b202b0c23c6063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59363ae7d1b04a2096422a604565aac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52e5246c37654fc1a7ec18c1a7cb22de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87831eb8ee694cddb843f57808e26858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e3fed6359b64794a28078a995eb8f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91ab6c88080f4e029e9f6007055509da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a592ee6d5c6047c485ee2768bd32f34c",
              "IPY_MODEL_abf14a702ad14f7e81862047377ea766",
              "IPY_MODEL_5197e417e5d0443f9a7b68983ae3d929"
            ],
            "layout": "IPY_MODEL_fb1385d70d2b44938a9032825b305ffd"
          }
        },
        "a592ee6d5c6047c485ee2768bd32f34c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e4e393388924540ba5144003b583b01",
            "placeholder": "​",
            "style": "IPY_MODEL_050c9d43e4b040fa91fa57201f5e0f89",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.6JTSFB_1.0.0/imdb_reviews-unsupervised.tfrecord*...:   0%"
          }
        },
        "abf14a702ad14f7e81862047377ea766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53b06cc24855427e849abcc9a71fc0f8",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f8898e8bccb4e43b1d3e20049a2c3cd",
            "value": 50000
          }
        },
        "5197e417e5d0443f9a7b68983ae3d929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_021f89be5cd94e00a52813de55546e84",
            "placeholder": "​",
            "style": "IPY_MODEL_836ecae055bb44a9a8a4f2bf81317428",
            "value": " 0/50000 [00:00&lt;?, ? examples/s]"
          }
        },
        "fb1385d70d2b44938a9032825b305ffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "3e4e393388924540ba5144003b583b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "050c9d43e4b040fa91fa57201f5e0f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53b06cc24855427e849abcc9a71fc0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f8898e8bccb4e43b1d3e20049a2c3cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "021f89be5cd94e00a52813de55546e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836ecae055bb44a9a8a4f2bf81317428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "736323bd52ca44cd9ba96a1038e89f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efc87f45e9504e0dac5793d17e78c02e",
              "IPY_MODEL_067ad684c5804e288d4583bcef23d9ee",
              "IPY_MODEL_4744354f266c461cb5f53bf32164678b"
            ],
            "layout": "IPY_MODEL_8bcae6b5f530467ca86c30566bf62f39"
          }
        },
        "efc87f45e9504e0dac5793d17e78c02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f4f5ae00334c4d8849bb529a396647",
            "placeholder": "​",
            "style": "IPY_MODEL_c51a01d79634417ba0843471fca32a81",
            "value": "Dl Completed...: 100%"
          }
        },
        "067ad684c5804e288d4583bcef23d9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f742ba5746674dfebbad293a5ec59cf1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19badab1e68042f7a9ebeaa11552ce9b",
            "value": 1
          }
        },
        "4744354f266c461cb5f53bf32164678b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05bd118de32402c8bb30a2b27ec1136",
            "placeholder": "​",
            "style": "IPY_MODEL_28812612b3d3495089574057ebbc7341",
            "value": " 1/1 [00:04&lt;00:00,  4.26s/ url]"
          }
        },
        "8bcae6b5f530467ca86c30566bf62f39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f4f5ae00334c4d8849bb529a396647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c51a01d79634417ba0843471fca32a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f742ba5746674dfebbad293a5ec59cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "19badab1e68042f7a9ebeaa11552ce9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b05bd118de32402c8bb30a2b27ec1136": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28812612b3d3495089574057ebbc7341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99a98975921c4510925c450306e6ba65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7b3e31ca1b145289abd9849e0b81181",
              "IPY_MODEL_c1885d6dc5a94ec9aa0564ad77ade99d",
              "IPY_MODEL_e6c0ccf253fb4cf4b902579ea619f961"
            ],
            "layout": "IPY_MODEL_c41981ee44114351991a287b3e1bdb8d"
          }
        },
        "c7b3e31ca1b145289abd9849e0b81181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b46f7defdb974f5fa12afccc32037967",
            "placeholder": "​",
            "style": "IPY_MODEL_9a3737d9e95d4d9dbba6623232ea90c0",
            "value": "Dl Size...: 100%"
          }
        },
        "c1885d6dc5a94ec9aa0564ad77ade99d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aac1388e25084ec7a8e9738705a4ea2e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43d5702bc0904efd9f852471828e5c90",
            "value": 1
          }
        },
        "e6c0ccf253fb4cf4b902579ea619f961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be18a6016c2346b2adf0c8207ba4ff68",
            "placeholder": "​",
            "style": "IPY_MODEL_aa79519c3a734076b04b97548bc51f82",
            "value": " 80/80 [00:04&lt;00:00, 22.66 MiB/s]"
          }
        },
        "c41981ee44114351991a287b3e1bdb8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46f7defdb974f5fa12afccc32037967": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a3737d9e95d4d9dbba6623232ea90c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aac1388e25084ec7a8e9738705a4ea2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "43d5702bc0904efd9f852471828e5c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be18a6016c2346b2adf0c8207ba4ff68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa79519c3a734076b04b97548bc51f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88b079f8f8224214ae3722f9c6ad7b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_567c63241be14210b60e952872741447",
              "IPY_MODEL_7ab6109aec094380b3b0c8b8aabac28b",
              "IPY_MODEL_96cd9117e7504e2c8cdf598bcd3946d4"
            ],
            "layout": "IPY_MODEL_cd404390afb04ab5875829e2b48dc975"
          }
        },
        "567c63241be14210b60e952872741447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee8f6709bfbc42128f5b51d64df51ad6",
            "placeholder": "​",
            "style": "IPY_MODEL_220ccd43ece14219bee9d256a4d2fe02",
            "value": "Generating splits...: 100%"
          }
        },
        "7ab6109aec094380b3b0c8b8aabac28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c0fccda0fe84b5386a00b09772ef42c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74b02beef7c24b209d1190ba1efa3e8f",
            "value": 3
          }
        },
        "96cd9117e7504e2c8cdf598bcd3946d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44ec617b06144b4faafc3366984421b0",
            "placeholder": "​",
            "style": "IPY_MODEL_82f7a347afc34fd6a9dd691ec59b7965",
            "value": " 3/3 [00:48&lt;00:00, 14.85s/ splits]"
          }
        },
        "cd404390afb04ab5875829e2b48dc975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ee8f6709bfbc42128f5b51d64df51ad6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "220ccd43ece14219bee9d256a4d2fe02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c0fccda0fe84b5386a00b09772ef42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b02beef7c24b209d1190ba1efa3e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44ec617b06144b4faafc3366984421b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f7a347afc34fd6a9dd691ec59b7965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d932432b9644a6fab0d5586b6137968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcfa5a288fa240028ea2dd22acc9b154",
              "IPY_MODEL_164e4b2ad6f74dbd800f71f3113ebeed",
              "IPY_MODEL_710e37c467674c8590e0aa9cab563d92"
            ],
            "layout": "IPY_MODEL_c4e84628cf9d4010bb2e0c9c692abb16"
          }
        },
        "bcfa5a288fa240028ea2dd22acc9b154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ff1700db524593a9dccb8c34f35962",
            "placeholder": "​",
            "style": "IPY_MODEL_5ba594593f674442aac1e542726d6219",
            "value": "Generating train examples...:  88%"
          }
        },
        "164e4b2ad6f74dbd800f71f3113ebeed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c89137114e724146befa9a0ec5e9851e",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84167e5fec0047c6aa148af9c59d07bf",
            "value": 25000
          }
        },
        "710e37c467674c8590e0aa9cab563d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c4ee612f4fe477ea34f7088099f0a01",
            "placeholder": "​",
            "style": "IPY_MODEL_72ef4495522044ae82b738faa2081ec1",
            "value": " 22063/25000 [00:13&lt;00:01, 2776.10 examples/s]"
          }
        },
        "c4e84628cf9d4010bb2e0c9c692abb16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d4ff1700db524593a9dccb8c34f35962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba594593f674442aac1e542726d6219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c89137114e724146befa9a0ec5e9851e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84167e5fec0047c6aa148af9c59d07bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c4ee612f4fe477ea34f7088099f0a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72ef4495522044ae82b738faa2081ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19b553ed5e114d2fba0c869fcf92213a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30dc9071bcc6407e9466ce7816a55b9b",
              "IPY_MODEL_0603cafc6742415b91414e1a0dbe539a",
              "IPY_MODEL_9f8fbe30ec7b4ab88724a47a256a21cf"
            ],
            "layout": "IPY_MODEL_8317be93219143e7bfacacbe6762f265"
          }
        },
        "30dc9071bcc6407e9466ce7816a55b9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8e305dfa8e44f59b75c38f0efb264c",
            "placeholder": "​",
            "style": "IPY_MODEL_181d95e3a59b4ec29aeb4d5a5ce8bd44",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2N0P8Y_1.0.0/imdb_reviews-train.tfrecord*...:   0%"
          }
        },
        "0603cafc6742415b91414e1a0dbe539a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0cad9db7bf4b738ac01879e907e367",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5d364869c18445bb35ccf3d362dc234",
            "value": 25000
          }
        },
        "9f8fbe30ec7b4ab88724a47a256a21cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b745b6014e7a465a97fb95a1cc573a71",
            "placeholder": "​",
            "style": "IPY_MODEL_569ecfa713eb4e4db19a0d23a4112a27",
            "value": " 0/25000 [00:00&lt;?, ? examples/s]"
          }
        },
        "8317be93219143e7bfacacbe6762f265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "6c8e305dfa8e44f59b75c38f0efb264c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181d95e3a59b4ec29aeb4d5a5ce8bd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a0cad9db7bf4b738ac01879e907e367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d364869c18445bb35ccf3d362dc234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b745b6014e7a465a97fb95a1cc573a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569ecfa713eb4e4db19a0d23a4112a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9be75525342f4b22a25746690c0cec89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcedf10f4fd540f6a83798673bf6ab91",
              "IPY_MODEL_4d96a5f083a84a7cb085f5a5518a6052",
              "IPY_MODEL_1798d1f8bedb43729701f71c96947990"
            ],
            "layout": "IPY_MODEL_1bef8741a13f4868956f96f0441faa36"
          }
        },
        "bcedf10f4fd540f6a83798673bf6ab91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce6af1f316e74f509528cc5c0f8ea664",
            "placeholder": "​",
            "style": "IPY_MODEL_0e194d82e8a84cd8912965330e70e4fb",
            "value": "Generating test examples...:  82%"
          }
        },
        "4d96a5f083a84a7cb085f5a5518a6052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1544c598b94887bc89a641025866d4",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d71208f3bf3949d98efe37d3895cdec9",
            "value": 25000
          }
        },
        "1798d1f8bedb43729701f71c96947990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10fed892a7c744778b43f6c81cf47246",
            "placeholder": "​",
            "style": "IPY_MODEL_a6de6d81b6f04f4a8af56cc87e5f025c",
            "value": " 20432/25000 [00:07&lt;00:01, 3616.69 examples/s]"
          }
        },
        "1bef8741a13f4868956f96f0441faa36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ce6af1f316e74f509528cc5c0f8ea664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e194d82e8a84cd8912965330e70e4fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef1544c598b94887bc89a641025866d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d71208f3bf3949d98efe37d3895cdec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10fed892a7c744778b43f6c81cf47246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6de6d81b6f04f4a8af56cc87e5f025c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "067d3397edcc4ce5b1873fb117068b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1e3b766a5b0470fb4c8e77625ab6ef5",
              "IPY_MODEL_e14233a50d544ac0a900402e32bec654",
              "IPY_MODEL_d7496030c5e64efb8d04605ed4bc97ac"
            ],
            "layout": "IPY_MODEL_1863c6fc304b46a7bacec2e06a8ef079"
          }
        },
        "c1e3b766a5b0470fb4c8e77625ab6ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ccedb7506a4bb9ba349d580b7424d0",
            "placeholder": "​",
            "style": "IPY_MODEL_cdaa3017c09e416a9a363f00d5ef8196",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2N0P8Y_1.0.0/imdb_reviews-test.tfrecord*...:   0%"
          }
        },
        "e14233a50d544ac0a900402e32bec654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc34c55c923549108f614409ebfe7fc9",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_044e3fb45aca4959915b1861ad6ea659",
            "value": 25000
          }
        },
        "d7496030c5e64efb8d04605ed4bc97ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52e061702a2f47a681e623ae31c9bfdf",
            "placeholder": "​",
            "style": "IPY_MODEL_675e778baab74087b992c67416ef5190",
            "value": " 0/25000 [00:00&lt;?, ? examples/s]"
          }
        },
        "1863c6fc304b46a7bacec2e06a8ef079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "24ccedb7506a4bb9ba349d580b7424d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdaa3017c09e416a9a363f00d5ef8196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc34c55c923549108f614409ebfe7fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "044e3fb45aca4959915b1861ad6ea659": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "52e061702a2f47a681e623ae31c9bfdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675e778baab74087b992c67416ef5190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fa8a80649b04471ada7dfce2af11bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cab76a2e49c94bacb6466836ecc342e8",
              "IPY_MODEL_f21bd2e2300743fc9a067cdeba6b1198",
              "IPY_MODEL_e50259de98b845119603a0c38f3f2d32"
            ],
            "layout": "IPY_MODEL_0dc5cd7295804315abb1d1ef8f8bd922"
          }
        },
        "cab76a2e49c94bacb6466836ecc342e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0846b263a78e4851a524d79166b337be",
            "placeholder": "​",
            "style": "IPY_MODEL_c2d8e726179f464c9173481a15f138a1",
            "value": "Generating unsupervised examples...:  95%"
          }
        },
        "f21bd2e2300743fc9a067cdeba6b1198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b19b3b7eca734032ad4471d3d7fe2824",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_582347b719df4ef7a8cfbc80c7980b91",
            "value": 50000
          }
        },
        "e50259de98b845119603a0c38f3f2d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0acac1cfe91641d892d426972e0ac770",
            "placeholder": "​",
            "style": "IPY_MODEL_cc30713b08724574bd086ec2c73f2854",
            "value": " 47276/50000 [00:10&lt;00:00, 5779.11 examples/s]"
          }
        },
        "0dc5cd7295804315abb1d1ef8f8bd922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "0846b263a78e4851a524d79166b337be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d8e726179f464c9173481a15f138a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b19b3b7eca734032ad4471d3d7fe2824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582347b719df4ef7a8cfbc80c7980b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0acac1cfe91641d892d426972e0ac770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc30713b08724574bd086ec2c73f2854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd50c73a014b49eaa9922479bcfde64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f31e445b0e0e4e82895d4e459eb7c769",
              "IPY_MODEL_52e8a1781f3b4492bdf90d081b2364d1",
              "IPY_MODEL_7dec364fdb0742109d9e91fd7cd049a6"
            ],
            "layout": "IPY_MODEL_135d4bc3cdaa4564b76696d67c4725ea"
          }
        },
        "f31e445b0e0e4e82895d4e459eb7c769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6728c8dfd479499980c1822e27d4cee0",
            "placeholder": "​",
            "style": "IPY_MODEL_f04b99e04f7344c69c06653b91cb1f5c",
            "value": "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2N0P8Y_1.0.0/imdb_reviews-unsupervised.tfrecord*...:   0%"
          }
        },
        "52e8a1781f3b4492bdf90d081b2364d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91116e79cdb64a319fb56367300ef6c3",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56120c82e1c64ea480ed13bd5ce6d4a7",
            "value": 50000
          }
        },
        "7dec364fdb0742109d9e91fd7cd049a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a205f5356524fc2aa735f645472d7dd",
            "placeholder": "​",
            "style": "IPY_MODEL_04ce37bd5d474efda4a0ff81d43e3ed6",
            "value": " 0/50000 [00:00&lt;?, ? examples/s]"
          }
        },
        "135d4bc3cdaa4564b76696d67c4725ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "6728c8dfd479499980c1822e27d4cee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f04b99e04f7344c69c06653b91cb1f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91116e79cdb64a319fb56367300ef6c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56120c82e1c64ea480ed13bd5ce6d4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a205f5356524fc2aa735f645472d7dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ce37bd5d474efda4a0ff81d43e3ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "ce68e295a6bf434889adeeb3bb38f051",
            "56d3122950964c569417b7eb87378877",
            "4ee7815b968d4e0c8a83473655f77b15",
            "1175dead08b14e47b6b9ac9865f2b710",
            "1b8bfa3109a741b9854e61a57d075299",
            "6dbc3346e69b4e3c90523c569e37a077",
            "8e1dd402040a4815b7192d016b443ae1",
            "bef9f3eb56bd456792a7c8bf81acff91",
            "281b053b3550494481e31704b8acb7cd",
            "1e6d3b10bdba442b96efa303afc07f98",
            "f7272c7f4239427f87a6d1be96c38006",
            "5a7cb7c8ce4a454dbc14c23c520e706d",
            "5d0ae9efb1c94c1daf1c1a95f7faa348",
            "d051df38613048838b3186d1d6aa1827",
            "bd29ea47cbc64453965d905ba85067e5",
            "e0d9212f014d4ff795e90772b11f8fd1",
            "29097fad2fc84c89b8f18f575b51a44b",
            "991bc0421328428785bacd9aafb62eea",
            "b064ba3549454318af9f2abd4527392b",
            "8801ff9cf5504eedaef116c4f20bfb57",
            "72780f86f1c947b6b23cb765989f0c76",
            "8e84c7a503314ddebb75d2509065e4fa",
            "82b77af544cd44968c63d897bbadcc6f",
            "c5194eed43934e458535031c8f61f23b",
            "b865bbd0e30d4e6abf7d73c589d984c5",
            "ff5a29aa74964720ac68b51ed10f03fd",
            "8700f1f80b87457e91c52f5bfa9129b0",
            "444f4206d933448891ebac5cd0096720",
            "1433dc6d3e934e5484a3659e70420fb6",
            "83dd7c00e0974fb4b784dbd8fba83cbc",
            "346b4a40e5c34d9aa723c880e9a08f26",
            "c3904d121c184b5385d1b264b23c1811",
            "414aa29cdf5d473aa6d44a71fe1d501c",
            "da4cf1ca0542433a9345aa833941304a",
            "bab4e18e8ed142e78b9232730cd83e93",
            "fd4bf250b8ce4e45bcad10864021f7f5",
            "78ba1b4de00b4b24b05fe3940bf4268d",
            "36f19a3b8f984c4bb1b04bfbda348314",
            "753de67d93e947d291e9512f2959c4ef",
            "6f38ac500c35473bbe25c8640b72047a",
            "3b64b2129068401db13089485b7e9ba6",
            "6d3e43d85a954839af46373e47f100a6",
            "c257a7a3062a411ca17589cb95789a4e",
            "09e4d74310794fc78911ebdce58e07e4",
            "244b38d54dbb48efa91f69852dd369c5",
            "775b274eac994f7cba48a52a4a7fa9ce",
            "b3ae3738e77948c986f666706b890e3d",
            "97d02a9457674cf5b599433f4f61db9b",
            "74454e3ecadd4fdf873a2a9f06a64aec",
            "44c5bc7a2f23413ca79d131f72b4345d",
            "417f1f2c349d49919dcc7bf025a6aef5",
            "73c4a7e16df3474e983a0369b5ff4bb6",
            "f4c612c8851c4f459e987eba6674c7b2",
            "c71d356016e340eb8ef736e635bd2746",
            "a05ac2525ddf401da14ea447592e71be",
            "1948c44dcb02452d9348598966f5fa26",
            "d1c5627363de4f9d94fe9e50e523d7a2",
            "4e6fe70677be4780a4aa1c796a90a9dd",
            "c34a53c4bc604cb6a743b206e469fb72",
            "8da05ce15d4441f4addfb48fa009221b",
            "0e033c39bdee42d18f1c2f8dd60c43bf",
            "28d3086ee10141edb1ccd3b026cd1bf9",
            "6f51154fd8154ad49f8891cf7f818a40",
            "6bc80d615a1d4f05ac6142591f5bd1d0",
            "50616f5d2fb34aef8000a952108a2937",
            "c074447f2d4746da843ed76507e80bd4",
            "09e309f68b364eb98a445a03cd437892",
            "fdd65a3833fc439fa6b55013242fe88e",
            "a0ab907dd6ad44ecbf032ab54e59a56d",
            "6de95d16796342179990dea48c3210d8",
            "6ae39e67d4484079aca6184ce375bbff",
            "7ec6fe0577264791b906d25768d54f7a",
            "8cd84be806e64491bece277230343ccf",
            "819b23a743c4422bbc8c6967452ff5dd",
            "8424fa64043043a692632e2919e7af53",
            "44141aa1ab004b1dba55c5039de06d43",
            "4aa1005c6f1142fab2babfcf5d21b386",
            "401b0d18e34f4d1aa1c7c99f59fbb236",
            "a0bbba0f6cbb4e079c78daaf569b30ec",
            "9a2180a3d0fb46879955c6bdd4edf2dc",
            "fb50f678afc64849b045297314a74909",
            "14603c7012064cd0af76b6bf08466503",
            "298884d6a9c849d191a033570fd31d27",
            "7da30fbd54e94a1986b202b0c23c6063",
            "59363ae7d1b04a2096422a604565aac5",
            "52e5246c37654fc1a7ec18c1a7cb22de",
            "87831eb8ee694cddb843f57808e26858",
            "1e3fed6359b64794a28078a995eb8f07",
            "91ab6c88080f4e029e9f6007055509da",
            "a592ee6d5c6047c485ee2768bd32f34c",
            "abf14a702ad14f7e81862047377ea766",
            "5197e417e5d0443f9a7b68983ae3d929",
            "fb1385d70d2b44938a9032825b305ffd",
            "3e4e393388924540ba5144003b583b01",
            "050c9d43e4b040fa91fa57201f5e0f89",
            "53b06cc24855427e849abcc9a71fc0f8",
            "8f8898e8bccb4e43b1d3e20049a2c3cd",
            "021f89be5cd94e00a52813de55546e84",
            "836ecae055bb44a9a8a4f2bf81317428"
          ]
        },
        "id": "6XTsY6SUJ6cY",
        "outputId": "aa156703-d253-4e1f-8716-c397d0dd1bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce68e295a6bf434889adeeb3bb38f051"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a7cb7c8ce4a454dbc14c23c520e706d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82b77af544cd44968c63d897bbadcc6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da4cf1ca0542433a9345aa833941304a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.6JTSFB_1.0.0/imdb_reviews-train.tfrecor…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "244b38d54dbb48efa91f69852dd369c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1948c44dcb02452d9348598966f5fa26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.6JTSFB_1.0.0/imdb_reviews-test.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e309f68b364eb98a445a03cd437892"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "401b0d18e34f4d1aa1c7c99f59fbb236"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.6JTSFB_1.0.0/imdb_reviews-unsupervised.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91ab6c88080f4e029e9f6007055509da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6715 | Val Loss: 0.6636 | Accuracy: 0.6058 | Precision: 0.5822 | Recall: 0.6621\n",
            "Epoch 02 | Training Loss: 0.6625 | Val Loss: 0.6634 | Accuracy: 0.6056 | Precision: 0.5792 | Recall: 0.6819\n",
            "Epoch 03 | Training Loss: 0.6604 | Val Loss: 0.6619 | Accuracy: 0.6032 | Precision: 0.5742 | Recall: 0.7021\n",
            "Epoch 04 | Training Loss: 0.6580 | Val Loss: 0.6625 | Accuracy: 0.6030 | Precision: 0.5717 | Recall: 0.7224\n",
            "Epoch 05 | Training Loss: 0.6582 | Val Loss: 0.6606 | Accuracy: 0.6084 | Precision: 0.5822 | Recall: 0.6811\n",
            "Epoch 06 | Training Loss: 0.6546 | Val Loss: 0.6683 | Accuracy: 0.5986 | Precision: 0.5630 | Recall: 0.7690\n",
            "Epoch 07 | Training Loss: 0.6536 | Val Loss: 0.6748 | Accuracy: 0.5810 | Precision: 0.5454 | Recall: 0.8152\n",
            "Epoch 08 | Training Loss: 0.6519 | Val Loss: 0.6611 | Accuracy: 0.6068 | Precision: 0.5787 | Recall: 0.6947\n",
            "Epoch 09 | Training Loss: 0.6481 | Val Loss: 0.6590 | Accuracy: 0.6064 | Precision: 0.5981 | Recall: 0.5734\n",
            "Epoch 10 | Training Loss: 0.6454 | Val Loss: 0.6654 | Accuracy: 0.6028 | Precision: 0.5696 | Recall: 0.7393\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6614 | Test Accuracy: 0.6064 | Test Precision: 0.5839 | Test Recall: 0.7410\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function tanh with an optimizer Adam"
      ],
      "metadata": {
        "id": "DZf-ehOPM4-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.tanh(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.tanh(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkkgxHPvOCsI",
        "outputId": "06208270-8944-4ec9-c68a-6207c864bc31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6832 | Val Loss: 0.6752 | Accuracy: 0.5882 | Precision: 0.5774 | Recall: 0.5615\n",
            "Epoch 02 | Training Loss: 0.6698 | Val Loss: 0.6681 | Accuracy: 0.6044 | Precision: 0.5820 | Recall: 0.6531\n",
            "Epoch 03 | Training Loss: 0.6646 | Val Loss: 0.6655 | Accuracy: 0.6060 | Precision: 0.5795 | Recall: 0.6823\n",
            "Epoch 04 | Training Loss: 0.6626 | Val Loss: 0.6640 | Accuracy: 0.6108 | Precision: 0.5876 | Recall: 0.6613\n",
            "Epoch 05 | Training Loss: 0.6614 | Val Loss: 0.6634 | Accuracy: 0.6120 | Precision: 0.5880 | Recall: 0.6671\n",
            "Epoch 06 | Training Loss: 0.6608 | Val Loss: 0.6652 | Accuracy: 0.6072 | Precision: 0.5750 | Recall: 0.7277\n",
            "Epoch 07 | Training Loss: 0.6608 | Val Loss: 0.6625 | Accuracy: 0.6088 | Precision: 0.5901 | Recall: 0.6324\n",
            "Epoch 08 | Training Loss: 0.6599 | Val Loss: 0.6620 | Accuracy: 0.6080 | Precision: 0.5897 | Recall: 0.6291\n",
            "Epoch 09 | Training Loss: 0.6594 | Val Loss: 0.6619 | Accuracy: 0.6106 | Precision: 0.5889 | Recall: 0.6518\n",
            "Epoch 10 | Training Loss: 0.6593 | Val Loss: 0.6645 | Accuracy: 0.6070 | Precision: 0.5751 | Recall: 0.7252\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6612 | Test Accuracy: 0.6044 | Test Precision: 0.5836 | Test Recall: 0.7293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rc57MUWVjOhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0005 and actiation function relu with an optimizer Adam , batach size=256"
      ],
      "metadata": {
        "id": "Od678Ix7SV_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kFsbgfBSwzE",
        "outputId": "f61da809-ff1e-4c1b-fd64-ca7d73369ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6776 | Val Loss: 0.6699 | Accuracy: 0.5930 | Precision: 0.6149 | Recall: 0.4295\n",
            "Epoch 02 | Training Loss: 0.6644 | Val Loss: 0.6634 | Accuracy: 0.6110 | Precision: 0.5905 | Recall: 0.6448\n",
            "Epoch 03 | Training Loss: 0.6602 | Val Loss: 0.6624 | Accuracy: 0.6084 | Precision: 0.5805 | Recall: 0.6931\n",
            "Epoch 04 | Training Loss: 0.6582 | Val Loss: 0.6616 | Accuracy: 0.6122 | Precision: 0.5882 | Recall: 0.6671\n",
            "Epoch 05 | Training Loss: 0.6563 | Val Loss: 0.6607 | Accuracy: 0.6100 | Precision: 0.5913 | Recall: 0.6333\n",
            "Epoch 06 | Training Loss: 0.6552 | Val Loss: 0.6623 | Accuracy: 0.6036 | Precision: 0.5744 | Recall: 0.7038\n",
            "Epoch 07 | Training Loss: 0.6542 | Val Loss: 0.6607 | Accuracy: 0.6108 | Precision: 0.5935 | Recall: 0.6258\n",
            "Epoch 08 | Training Loss: 0.6516 | Val Loss: 0.6599 | Accuracy: 0.6082 | Precision: 0.5890 | Recall: 0.6345\n",
            "Epoch 09 | Training Loss: 0.6499 | Val Loss: 0.6598 | Accuracy: 0.6052 | Precision: 0.5856 | Recall: 0.6349\n",
            "Epoch 10 | Training Loss: 0.6491 | Val Loss: 0.6622 | Accuracy: 0.6050 | Precision: 0.5747 | Recall: 0.7129\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6590 | Test Accuracy: 0.6088 | Test Precision: 0.5889 | Test Recall: 0.7206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer Adam , batach size=256\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkpv5G8oTf7Z",
        "outputId": "bd76ce10-13ad-4b1a-dde3-b77f57e7562a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6904 | Val Loss: 0.6850 | Accuracy: 0.5576 | Precision: 0.5347 | Recall: 0.6737\n",
            "Epoch 02 | Training Loss: 0.6790 | Val Loss: 0.6770 | Accuracy: 0.5872 | Precision: 0.5687 | Recall: 0.6147\n",
            "Epoch 03 | Training Loss: 0.6718 | Val Loss: 0.6720 | Accuracy: 0.5988 | Precision: 0.5709 | Recall: 0.6939\n",
            "Epoch 04 | Training Loss: 0.6672 | Val Loss: 0.6683 | Accuracy: 0.6044 | Precision: 0.5779 | Recall: 0.6823\n",
            "Epoch 05 | Training Loss: 0.6635 | Val Loss: 0.6669 | Accuracy: 0.6036 | Precision: 0.5744 | Recall: 0.7038\n",
            "Epoch 06 | Training Loss: 0.6617 | Val Loss: 0.6666 | Accuracy: 0.6018 | Precision: 0.5709 | Recall: 0.7195\n",
            "Epoch 07 | Training Loss: 0.6603 | Val Loss: 0.6648 | Accuracy: 0.6068 | Precision: 0.5792 | Recall: 0.6910\n",
            "Epoch 08 | Training Loss: 0.6595 | Val Loss: 0.6641 | Accuracy: 0.6100 | Precision: 0.5832 | Recall: 0.6856\n",
            "Epoch 09 | Training Loss: 0.6586 | Val Loss: 0.6638 | Accuracy: 0.6092 | Precision: 0.5823 | Recall: 0.6861\n",
            "Epoch 10 | Training Loss: 0.6581 | Val Loss: 0.6652 | Accuracy: 0.6004 | Precision: 0.5700 | Recall: 0.7158\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6615 | Test Accuracy: 0.6051 | Test Precision: 0.5850 | Test Recall: 0.7232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function relu with an optimizer Adam , batach size=256\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG7cJ2n7UPrJ",
        "outputId": "94187c91-41f3-4db6-a9d7-0596715bcb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6728 | Val Loss: 0.6675 | Accuracy: 0.5964 | Precision: 0.5943 | Recall: 0.5276\n",
            "Epoch 02 | Training Loss: 0.6619 | Val Loss: 0.6676 | Accuracy: 0.5958 | Precision: 0.5615 | Recall: 0.7587\n",
            "Epoch 03 | Training Loss: 0.6595 | Val Loss: 0.6613 | Accuracy: 0.6098 | Precision: 0.5867 | Recall: 0.6605\n",
            "Epoch 04 | Training Loss: 0.6582 | Val Loss: 0.6617 | Accuracy: 0.6052 | Precision: 0.5751 | Recall: 0.7112\n",
            "Epoch 05 | Training Loss: 0.6548 | Val Loss: 0.6610 | Accuracy: 0.6080 | Precision: 0.6058 | Recall: 0.5479\n",
            "Epoch 06 | Training Loss: 0.6517 | Val Loss: 0.6627 | Accuracy: 0.6038 | Precision: 0.5774 | Recall: 0.6819\n",
            "Epoch 07 | Training Loss: 0.6507 | Val Loss: 0.6596 | Accuracy: 0.6114 | Precision: 0.5977 | Recall: 0.6068\n",
            "Epoch 08 | Training Loss: 0.6478 | Val Loss: 0.6582 | Accuracy: 0.6086 | Precision: 0.5862 | Recall: 0.6551\n",
            "Epoch 09 | Training Loss: 0.6451 | Val Loss: 0.6577 | Accuracy: 0.6126 | Precision: 0.5912 | Recall: 0.6510\n",
            "Epoch 10 | Training Loss: 0.6423 | Val Loss: 0.6661 | Accuracy: 0.6000 | Precision: 0.5670 | Recall: 0.7401\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6608 | Test Accuracy: 0.6064 | Test Precision: 0.5838 | Test Recall: 0.7410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655  with 4 hidden layers\n",
        "#Learning Rate of 0.001 and actiation function relu with an optimizer Adam , batach size=256\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3,size_hidden4, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_hidden4=size_hidden4\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "\n",
        "        self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output], stddev=0.1))\n",
        "        self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3,self.W4,self.b1, self.b2, self.b3,self.b4]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        ##hidden layer\n",
        "        h3 = tf.matmul(z2, self.W3) + self.b3\n",
        "        z3 = tf.nn.relu(h3)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z3, self.W4) + self.b4\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_hidden4=16\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_hidden4, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkqPSgiHVJk2",
        "outputId": "957a9fb9-20f1-4d50-db0a-f741bec5ffbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6744 | Val Loss: 0.6652 | Accuracy: 0.6034 | Precision: 0.6011 | Recall: 0.5408\n",
            "Epoch 02 | Training Loss: 0.6627 | Val Loss: 0.6638 | Accuracy: 0.6048 | Precision: 0.5721 | Recall: 0.7331\n",
            "Epoch 03 | Training Loss: 0.6605 | Val Loss: 0.6603 | Accuracy: 0.6088 | Precision: 0.5816 | Recall: 0.6881\n",
            "Epoch 04 | Training Loss: 0.6580 | Val Loss: 0.6601 | Accuracy: 0.6102 | Precision: 0.5827 | Recall: 0.6906\n",
            "Epoch 05 | Training Loss: 0.6550 | Val Loss: 0.6598 | Accuracy: 0.6086 | Precision: 0.6063 | Recall: 0.5495\n",
            "Epoch 06 | Training Loss: 0.6522 | Val Loss: 0.6623 | Accuracy: 0.6086 | Precision: 0.5825 | Recall: 0.6799\n",
            "Epoch 07 | Training Loss: 0.6513 | Val Loss: 0.6587 | Accuracy: 0.6142 | Precision: 0.6047 | Recall: 0.5895\n",
            "Epoch 08 | Training Loss: 0.6469 | Val Loss: 0.6569 | Accuracy: 0.6074 | Precision: 0.5858 | Recall: 0.6489\n",
            "Epoch 09 | Training Loss: 0.6443 | Val Loss: 0.6571 | Accuracy: 0.6074 | Precision: 0.5931 | Recall: 0.6056\n",
            "Epoch 10 | Training Loss: 0.6417 | Val Loss: 0.6665 | Accuracy: 0.5986 | Precision: 0.5646 | Recall: 0.7517\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6614 | Test Accuracy: 0.6066 | Test Precision: 0.5816 | Test Recall: 0.7599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655  with 4 hidden layers\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer Adam , batach size=256\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3,size_hidden4, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_hidden4=size_hidden4\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "\n",
        "        self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output], stddev=0.1))\n",
        "        self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3,self.W4,self.b1, self.b2, self.b3,self.b4]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        ##hidden layer\n",
        "        h3 = tf.matmul(z2, self.W3) + self.b3\n",
        "        z3 = tf.nn.relu(h3)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z3, self.W4) + self.b4\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_hidden4=16\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_hidden4, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnjzx24Zkwet",
        "outputId": "30576e49-4fa3-41d8-d0ba-dcaf44c90424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6894 | Val Loss: 0.6860 | Accuracy: 0.5674 | Precision: 0.5399 | Recall: 0.7277\n",
            "Epoch 02 | Training Loss: 0.6819 | Val Loss: 0.6786 | Accuracy: 0.5900 | Precision: 0.5714 | Recall: 0.6176\n",
            "Epoch 03 | Training Loss: 0.6745 | Val Loss: 0.6727 | Accuracy: 0.5956 | Precision: 0.5691 | Recall: 0.6832\n",
            "Epoch 04 | Training Loss: 0.6688 | Val Loss: 0.6685 | Accuracy: 0.6018 | Precision: 0.5736 | Recall: 0.6964\n",
            "Epoch 05 | Training Loss: 0.6643 | Val Loss: 0.6660 | Accuracy: 0.6042 | Precision: 0.5758 | Recall: 0.6972\n",
            "Epoch 06 | Training Loss: 0.6620 | Val Loss: 0.6652 | Accuracy: 0.6052 | Precision: 0.5743 | Recall: 0.7174\n",
            "Epoch 07 | Training Loss: 0.6601 | Val Loss: 0.6635 | Accuracy: 0.6104 | Precision: 0.5832 | Recall: 0.6885\n",
            "Epoch 08 | Training Loss: 0.6591 | Val Loss: 0.6629 | Accuracy: 0.6116 | Precision: 0.5847 | Recall: 0.6865\n",
            "Epoch 09 | Training Loss: 0.6581 | Val Loss: 0.6626 | Accuracy: 0.6130 | Precision: 0.5853 | Recall: 0.6918\n",
            "Epoch 10 | Training Loss: 0.6574 | Val Loss: 0.6637 | Accuracy: 0.6046 | Precision: 0.5738 | Recall: 0.7166\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6606 | Test Accuracy: 0.6075 | Test Precision: 0.5878 | Test Recall: 0.7195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655  with 4 hidden layers\n",
        "#Learning Rate of 0.0005 and actiation function relu with an optimizer Adam , batach size=256\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3,size_hidden4, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_hidden4=size_hidden4\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "\n",
        "        self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output], stddev=0.1))\n",
        "        self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3,self.W4,self.b1, self.b2, self.b3,self.b4]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        ##hidden layer\n",
        "        h3 = tf.matmul(z2, self.W3) + self.b3\n",
        "        z3 = tf.nn.relu(h3)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z3, self.W4) + self.b4\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_hidden4=16\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_hidden4, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXwyBRjodB8x",
        "outputId": "db1ad9a4-5207-4ba1-e09b-ab099e235fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6791 | Val Loss: 0.6699 | Accuracy: 0.5862 | Precision: 0.6099 | Recall: 0.4064\n",
            "Epoch 02 | Training Loss: 0.6647 | Val Loss: 0.6628 | Accuracy: 0.6104 | Precision: 0.5885 | Recall: 0.6531\n",
            "Epoch 03 | Training Loss: 0.6605 | Val Loss: 0.6611 | Accuracy: 0.6124 | Precision: 0.5894 | Recall: 0.6609\n",
            "Epoch 04 | Training Loss: 0.6583 | Val Loss: 0.6605 | Accuracy: 0.6092 | Precision: 0.5880 | Recall: 0.6477\n",
            "Epoch 05 | Training Loss: 0.6557 | Val Loss: 0.6591 | Accuracy: 0.6100 | Precision: 0.5925 | Recall: 0.6262\n",
            "Epoch 06 | Training Loss: 0.6541 | Val Loss: 0.6612 | Accuracy: 0.6056 | Precision: 0.5776 | Recall: 0.6943\n",
            "Epoch 07 | Training Loss: 0.6532 | Val Loss: 0.6589 | Accuracy: 0.6098 | Precision: 0.5971 | Recall: 0.5998\n",
            "Epoch 08 | Training Loss: 0.6496 | Val Loss: 0.6591 | Accuracy: 0.6048 | Precision: 0.5871 | Recall: 0.6229\n",
            "Epoch 09 | Training Loss: 0.6478 | Val Loss: 0.6580 | Accuracy: 0.6070 | Precision: 0.5903 | Recall: 0.6188\n",
            "Epoch 10 | Training Loss: 0.6456 | Val Loss: 0.6675 | Accuracy: 0.5956 | Precision: 0.5615 | Recall: 0.7570\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6611 | Test Accuracy: 0.6065 | Test Precision: 0.5809 | Test Recall: 0.7648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function relu with an optimizer Adam , batach size=512\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl_WKEl-mvFu",
        "outputId": "02547bd2-3de2-4386-880e-98547de4d25d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6792 | Val Loss: 0.6682 | Accuracy: 0.5958 | Precision: 0.6053 | Recall: 0.4777\n",
            "Epoch 02 | Training Loss: 0.6665 | Val Loss: 0.6636 | Accuracy: 0.6098 | Precision: 0.5943 | Recall: 0.6151\n",
            "Epoch 03 | Training Loss: 0.6591 | Val Loss: 0.6617 | Accuracy: 0.6144 | Precision: 0.5899 | Recall: 0.6712\n",
            "Epoch 04 | Training Loss: 0.6578 | Val Loss: 0.6609 | Accuracy: 0.6140 | Precision: 0.5890 | Recall: 0.6741\n",
            "Epoch 05 | Training Loss: 0.6554 | Val Loss: 0.6603 | Accuracy: 0.6122 | Precision: 0.5937 | Recall: 0.6337\n",
            "Epoch 06 | Training Loss: 0.6576 | Val Loss: 0.6603 | Accuracy: 0.6068 | Precision: 0.5821 | Recall: 0.6700\n",
            "Epoch 07 | Training Loss: 0.6522 | Val Loss: 0.6605 | Accuracy: 0.6104 | Precision: 0.5840 | Recall: 0.6828\n",
            "Epoch 08 | Training Loss: 0.6508 | Val Loss: 0.6595 | Accuracy: 0.6120 | Precision: 0.5975 | Recall: 0.6118\n",
            "Epoch 09 | Training Loss: 0.6486 | Val Loss: 0.6594 | Accuracy: 0.6124 | Precision: 0.6031 | Recall: 0.5866\n",
            "Epoch 10 | Training Loss: 0.6476 | Val Loss: 0.6607 | Accuracy: 0.6056 | Precision: 0.5780 | Recall: 0.6906\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6577 | Test Accuracy: 0.6097 | Test Precision: 0.5933 | Test Recall: 0.6978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer Adam , batach size=512\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mink2Qgwpw5E",
        "outputId": "aa793945-2438-43d0-e5df-f52f318e2610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6946 | Val Loss: 0.6866 | Accuracy: 0.5476 | Precision: 0.5359 | Recall: 0.4992\n",
            "Epoch 02 | Training Loss: 0.6835 | Val Loss: 0.6830 | Accuracy: 0.5714 | Precision: 0.5571 | Recall: 0.5656\n",
            "Epoch 03 | Training Loss: 0.6791 | Val Loss: 0.6798 | Accuracy: 0.5788 | Precision: 0.5536 | Recall: 0.6770\n",
            "Epoch 04 | Training Loss: 0.6749 | Val Loss: 0.6751 | Accuracy: 0.5910 | Precision: 0.5675 | Recall: 0.6572\n",
            "Epoch 05 | Training Loss: 0.6705 | Val Loss: 0.6722 | Accuracy: 0.5990 | Precision: 0.5713 | Recall: 0.6922\n",
            "Epoch 06 | Training Loss: 0.6674 | Val Loss: 0.6694 | Accuracy: 0.6010 | Precision: 0.5745 | Recall: 0.6828\n",
            "Epoch 07 | Training Loss: 0.6648 | Val Loss: 0.6686 | Accuracy: 0.6022 | Precision: 0.5722 | Recall: 0.7108\n",
            "Epoch 08 | Training Loss: 0.6630 | Val Loss: 0.6668 | Accuracy: 0.6056 | Precision: 0.5785 | Recall: 0.6873\n",
            "Epoch 09 | Training Loss: 0.6616 | Val Loss: 0.6658 | Accuracy: 0.6070 | Precision: 0.5806 | Recall: 0.6819\n",
            "Epoch 10 | Training Loss: 0.6605 | Val Loss: 0.6661 | Accuracy: 0.6034 | Precision: 0.5742 | Recall: 0.7038\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6628 | Test Accuracy: 0.6064 | Test Precision: 0.5887 | Test Recall: 0.7056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function leakyrelu with an optimizer LeakyRelu , batach size=512\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMZksSZFqV0d",
        "outputId": "04701422-2559-4b23-b709-65d45511d82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6935 | Val Loss: 0.6872 | Accuracy: 0.5514 | Precision: 0.5326 | Recall: 0.6101\n",
            "Epoch 02 | Training Loss: 0.6828 | Val Loss: 0.6817 | Accuracy: 0.5712 | Precision: 0.5582 | Recall: 0.5536\n",
            "Epoch 03 | Training Loss: 0.6780 | Val Loss: 0.6786 | Accuracy: 0.5800 | Precision: 0.5553 | Recall: 0.6716\n",
            "Epoch 04 | Training Loss: 0.6738 | Val Loss: 0.6744 | Accuracy: 0.5890 | Precision: 0.5645 | Recall: 0.6663\n",
            "Epoch 05 | Training Loss: 0.6702 | Val Loss: 0.6716 | Accuracy: 0.5968 | Precision: 0.5690 | Recall: 0.6939\n",
            "Epoch 06 | Training Loss: 0.6675 | Val Loss: 0.6689 | Accuracy: 0.6026 | Precision: 0.5748 | Recall: 0.6927\n",
            "Epoch 07 | Training Loss: 0.6650 | Val Loss: 0.6679 | Accuracy: 0.6044 | Precision: 0.5741 | Recall: 0.7125\n",
            "Epoch 08 | Training Loss: 0.6633 | Val Loss: 0.6658 | Accuracy: 0.6048 | Precision: 0.5787 | Recall: 0.6799\n",
            "Epoch 09 | Training Loss: 0.6619 | Val Loss: 0.6650 | Accuracy: 0.6050 | Precision: 0.5783 | Recall: 0.6840\n",
            "Epoch 10 | Training Loss: 0.6608 | Val Loss: 0.6653 | Accuracy: 0.6068 | Precision: 0.5771 | Recall: 0.7075\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6625 | Test Accuracy: 0.6055 | Test Precision: 0.5876 | Test Recall: 0.7077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function tanh with an optimizer LeakyRelu , batach size=512\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.tanh(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.tanh(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "f3udW8LOr0cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.tanh(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.tanh(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "736323bd52ca44cd9ba96a1038e89f37",
            "efc87f45e9504e0dac5793d17e78c02e",
            "067ad684c5804e288d4583bcef23d9ee",
            "4744354f266c461cb5f53bf32164678b",
            "8bcae6b5f530467ca86c30566bf62f39",
            "d8f4f5ae00334c4d8849bb529a396647",
            "c51a01d79634417ba0843471fca32a81",
            "f742ba5746674dfebbad293a5ec59cf1",
            "19badab1e68042f7a9ebeaa11552ce9b",
            "b05bd118de32402c8bb30a2b27ec1136",
            "28812612b3d3495089574057ebbc7341",
            "99a98975921c4510925c450306e6ba65",
            "c7b3e31ca1b145289abd9849e0b81181",
            "c1885d6dc5a94ec9aa0564ad77ade99d",
            "e6c0ccf253fb4cf4b902579ea619f961",
            "c41981ee44114351991a287b3e1bdb8d",
            "b46f7defdb974f5fa12afccc32037967",
            "9a3737d9e95d4d9dbba6623232ea90c0",
            "aac1388e25084ec7a8e9738705a4ea2e",
            "43d5702bc0904efd9f852471828e5c90",
            "be18a6016c2346b2adf0c8207ba4ff68",
            "aa79519c3a734076b04b97548bc51f82",
            "88b079f8f8224214ae3722f9c6ad7b3e",
            "567c63241be14210b60e952872741447",
            "7ab6109aec094380b3b0c8b8aabac28b",
            "96cd9117e7504e2c8cdf598bcd3946d4",
            "cd404390afb04ab5875829e2b48dc975",
            "ee8f6709bfbc42128f5b51d64df51ad6",
            "220ccd43ece14219bee9d256a4d2fe02",
            "7c0fccda0fe84b5386a00b09772ef42c",
            "74b02beef7c24b209d1190ba1efa3e8f",
            "44ec617b06144b4faafc3366984421b0",
            "82f7a347afc34fd6a9dd691ec59b7965",
            "4d932432b9644a6fab0d5586b6137968",
            "bcfa5a288fa240028ea2dd22acc9b154",
            "164e4b2ad6f74dbd800f71f3113ebeed",
            "710e37c467674c8590e0aa9cab563d92",
            "c4e84628cf9d4010bb2e0c9c692abb16",
            "d4ff1700db524593a9dccb8c34f35962",
            "5ba594593f674442aac1e542726d6219",
            "c89137114e724146befa9a0ec5e9851e",
            "84167e5fec0047c6aa148af9c59d07bf",
            "3c4ee612f4fe477ea34f7088099f0a01",
            "72ef4495522044ae82b738faa2081ec1",
            "19b553ed5e114d2fba0c869fcf92213a",
            "30dc9071bcc6407e9466ce7816a55b9b",
            "0603cafc6742415b91414e1a0dbe539a",
            "9f8fbe30ec7b4ab88724a47a256a21cf",
            "8317be93219143e7bfacacbe6762f265",
            "6c8e305dfa8e44f59b75c38f0efb264c",
            "181d95e3a59b4ec29aeb4d5a5ce8bd44",
            "0a0cad9db7bf4b738ac01879e907e367",
            "b5d364869c18445bb35ccf3d362dc234",
            "b745b6014e7a465a97fb95a1cc573a71",
            "569ecfa713eb4e4db19a0d23a4112a27",
            "9be75525342f4b22a25746690c0cec89",
            "bcedf10f4fd540f6a83798673bf6ab91",
            "4d96a5f083a84a7cb085f5a5518a6052",
            "1798d1f8bedb43729701f71c96947990",
            "1bef8741a13f4868956f96f0441faa36",
            "ce6af1f316e74f509528cc5c0f8ea664",
            "0e194d82e8a84cd8912965330e70e4fb",
            "ef1544c598b94887bc89a641025866d4",
            "d71208f3bf3949d98efe37d3895cdec9",
            "10fed892a7c744778b43f6c81cf47246",
            "a6de6d81b6f04f4a8af56cc87e5f025c",
            "067d3397edcc4ce5b1873fb117068b01",
            "c1e3b766a5b0470fb4c8e77625ab6ef5",
            "e14233a50d544ac0a900402e32bec654",
            "d7496030c5e64efb8d04605ed4bc97ac",
            "1863c6fc304b46a7bacec2e06a8ef079",
            "24ccedb7506a4bb9ba349d580b7424d0",
            "cdaa3017c09e416a9a363f00d5ef8196",
            "dc34c55c923549108f614409ebfe7fc9",
            "044e3fb45aca4959915b1861ad6ea659",
            "52e061702a2f47a681e623ae31c9bfdf",
            "675e778baab74087b992c67416ef5190",
            "1fa8a80649b04471ada7dfce2af11bac",
            "cab76a2e49c94bacb6466836ecc342e8",
            "f21bd2e2300743fc9a067cdeba6b1198",
            "e50259de98b845119603a0c38f3f2d32",
            "0dc5cd7295804315abb1d1ef8f8bd922",
            "0846b263a78e4851a524d79166b337be",
            "c2d8e726179f464c9173481a15f138a1",
            "b19b3b7eca734032ad4471d3d7fe2824",
            "582347b719df4ef7a8cfbc80c7980b91",
            "0acac1cfe91641d892d426972e0ac770",
            "cc30713b08724574bd086ec2c73f2854",
            "bd50c73a014b49eaa9922479bcfde64d",
            "f31e445b0e0e4e82895d4e459eb7c769",
            "52e8a1781f3b4492bdf90d081b2364d1",
            "7dec364fdb0742109d9e91fd7cd049a6",
            "135d4bc3cdaa4564b76696d67c4725ea",
            "6728c8dfd479499980c1822e27d4cee0",
            "f04b99e04f7344c69c06653b91cb1f5c",
            "91116e79cdb64a319fb56367300ef6c3",
            "56120c82e1c64ea480ed13bd5ce6d4a7",
            "5a205f5356524fc2aa735f645472d7dd",
            "04ce37bd5d474efda4a0ff81d43e3ed6"
          ]
        },
        "id": "iNWPwTDDjRn_",
        "outputId": "3d05da59-45e2-4e16-d440-9aefdb8695c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "736323bd52ca44cd9ba96a1038e89f37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99a98975921c4510925c450306e6ba65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88b079f8f8224214ae3722f9c6ad7b3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d932432b9644a6fab0d5586b6137968"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2N0P8Y_1.0.0/imdb_reviews-train.tfrecor…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19b553ed5e114d2fba0c869fcf92213a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9be75525342f4b22a25746690c0cec89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2N0P8Y_1.0.0/imdb_reviews-test.tfrecord…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "067d3397edcc4ce5b1873fb117068b01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1fa8a80649b04471ada7dfce2af11bac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/incomplete.2N0P8Y_1.0.0/imdb_reviews-unsupervised.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd50c73a014b49eaa9922479bcfde64d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6832 | Val Loss: 0.6752 | Accuracy: 0.5882 | Precision: 0.5774 | Recall: 0.5615\n",
            "Epoch 02 | Training Loss: 0.6698 | Val Loss: 0.6681 | Accuracy: 0.6044 | Precision: 0.5820 | Recall: 0.6531\n",
            "Epoch 03 | Training Loss: 0.6646 | Val Loss: 0.6655 | Accuracy: 0.6060 | Precision: 0.5795 | Recall: 0.6823\n",
            "Epoch 04 | Training Loss: 0.6626 | Val Loss: 0.6640 | Accuracy: 0.6108 | Precision: 0.5876 | Recall: 0.6613\n",
            "Epoch 05 | Training Loss: 0.6614 | Val Loss: 0.6634 | Accuracy: 0.6120 | Precision: 0.5880 | Recall: 0.6671\n",
            "Epoch 06 | Training Loss: 0.6608 | Val Loss: 0.6652 | Accuracy: 0.6072 | Precision: 0.5750 | Recall: 0.7277\n",
            "Epoch 07 | Training Loss: 0.6608 | Val Loss: 0.6625 | Accuracy: 0.6088 | Precision: 0.5901 | Recall: 0.6324\n",
            "Epoch 08 | Training Loss: 0.6599 | Val Loss: 0.6620 | Accuracy: 0.6080 | Precision: 0.5897 | Recall: 0.6291\n",
            "Epoch 09 | Training Loss: 0.6594 | Val Loss: 0.6619 | Accuracy: 0.6106 | Precision: 0.5889 | Recall: 0.6518\n",
            "Epoch 10 | Training Loss: 0.6593 | Val Loss: 0.6645 | Accuracy: 0.6070 | Precision: 0.5751 | Recall: 0.7252\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6612 | Test Accuracy: 0.6044 | Test Precision: 0.5836 | Test Recall: 0.7293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer SGD , batach size=256\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF0hEyJOk3l7",
        "outputId": "26a92916-3fa6-4302-832a-0af5dc0a13f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.7191 | Val Loss: 0.7246 | Accuracy: 0.4852 | Precision: 0.4850 | Recall: 1.0000\n",
            "Epoch 02 | Training Loss: 0.7102 | Val Loss: 0.7157 | Accuracy: 0.4848 | Precision: 0.4848 | Recall: 0.9967\n",
            "Epoch 03 | Training Loss: 0.7039 | Val Loss: 0.7091 | Accuracy: 0.4872 | Precision: 0.4858 | Recall: 0.9909\n",
            "Epoch 04 | Training Loss: 0.6994 | Val Loss: 0.7043 | Accuracy: 0.4902 | Precision: 0.4871 | Recall: 0.9707\n",
            "Epoch 05 | Training Loss: 0.6963 | Val Loss: 0.7007 | Accuracy: 0.4962 | Precision: 0.4898 | Recall: 0.9439\n",
            "Epoch 06 | Training Loss: 0.6940 | Val Loss: 0.6981 | Accuracy: 0.5002 | Precision: 0.4916 | Recall: 0.9055\n",
            "Epoch 07 | Training Loss: 0.6924 | Val Loss: 0.6961 | Accuracy: 0.5068 | Precision: 0.4951 | Recall: 0.8721\n",
            "Epoch 08 | Training Loss: 0.6913 | Val Loss: 0.6947 | Accuracy: 0.5142 | Precision: 0.4994 | Recall: 0.8383\n",
            "Epoch 09 | Training Loss: 0.6905 | Val Loss: 0.6935 | Accuracy: 0.5196 | Precision: 0.5028 | Recall: 0.8040\n",
            "Epoch 10 | Training Loss: 0.6899 | Val Loss: 0.6927 | Accuracy: 0.5264 | Precision: 0.5076 | Recall: 0.7710\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6907 | Test Accuracy: 0.5339 | Test Precision: 0.5233 | Test Recall: 0.7611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer RMSProp , batach size=128\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elA5N8__mCnw",
        "outputId": "9b23bbed-faac-419a-c457-23e5b65cc582"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6843 | Val Loss: 0.6778 | Accuracy: 0.5828 | Precision: 0.5796 | Recall: 0.5078\n",
            "Epoch 02 | Training Loss: 0.6729 | Val Loss: 0.6721 | Accuracy: 0.5992 | Precision: 0.5700 | Recall: 0.7050\n",
            "Epoch 03 | Training Loss: 0.6668 | Val Loss: 0.6669 | Accuracy: 0.6086 | Precision: 0.5946 | Recall: 0.6056\n",
            "Epoch 04 | Training Loss: 0.6633 | Val Loss: 0.6657 | Accuracy: 0.6062 | Precision: 0.5781 | Recall: 0.6947\n",
            "Epoch 05 | Training Loss: 0.6616 | Val Loss: 0.6642 | Accuracy: 0.6104 | Precision: 0.5900 | Recall: 0.6436\n",
            "Epoch 06 | Training Loss: 0.6604 | Val Loss: 0.6669 | Accuracy: 0.5996 | Precision: 0.5675 | Recall: 0.7323\n",
            "Epoch 07 | Training Loss: 0.6598 | Val Loss: 0.6647 | Accuracy: 0.6084 | Precision: 0.5789 | Recall: 0.7050\n",
            "Epoch 08 | Training Loss: 0.6590 | Val Loss: 0.6631 | Accuracy: 0.6144 | Precision: 0.5918 | Recall: 0.6597\n",
            "Epoch 09 | Training Loss: 0.6585 | Val Loss: 0.6629 | Accuracy: 0.6136 | Precision: 0.5923 | Recall: 0.6510\n",
            "Epoch 10 | Training Loss: 0.6577 | Val Loss: 0.6638 | Accuracy: 0.6110 | Precision: 0.5811 | Recall: 0.7079\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6606 | Test Accuracy: 0.6078 | Test Precision: 0.5904 | Test Recall: 0.7043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer RMSProp , batach size=512\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYrKg7ncmotY",
        "outputId": "d20d8591-14bc-4993-953e-dae1ac068731"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6909 | Val Loss: 0.6847 | Accuracy: 0.5538 | Precision: 0.5559 | Recall: 0.3960\n",
            "Epoch 02 | Training Loss: 0.6821 | Val Loss: 0.6817 | Accuracy: 0.5714 | Precision: 0.5506 | Recall: 0.6304\n",
            "Epoch 03 | Training Loss: 0.6776 | Val Loss: 0.6768 | Accuracy: 0.5860 | Precision: 0.5824 | Recall: 0.5161\n",
            "Epoch 04 | Training Loss: 0.6738 | Val Loss: 0.6738 | Accuracy: 0.5910 | Precision: 0.5752 | Recall: 0.5978\n",
            "Epoch 05 | Training Loss: 0.6705 | Val Loss: 0.6711 | Accuracy: 0.5988 | Precision: 0.5822 | Recall: 0.6106\n",
            "Epoch 06 | Training Loss: 0.6679 | Val Loss: 0.6733 | Accuracy: 0.5934 | Precision: 0.5590 | Recall: 0.7640\n",
            "Epoch 07 | Training Loss: 0.6663 | Val Loss: 0.6695 | Accuracy: 0.6012 | Precision: 0.5708 | Recall: 0.7149\n",
            "Epoch 08 | Training Loss: 0.6646 | Val Loss: 0.6674 | Accuracy: 0.6050 | Precision: 0.5773 | Recall: 0.6914\n",
            "Epoch 09 | Training Loss: 0.6634 | Val Loss: 0.6660 | Accuracy: 0.6078 | Precision: 0.5861 | Recall: 0.6502\n",
            "Epoch 10 | Training Loss: 0.6624 | Val Loss: 0.6658 | Accuracy: 0.6068 | Precision: 0.5804 | Recall: 0.6823\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6635 | Test Accuracy: 0.6037 | Test Precision: 0.5907 | Test Recall: 0.6750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.0001 and actiation function relu with an optimizer SGD , batach size=512\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf3TmT-zq7oh",
        "outputId": "f7d6d6b6-b022-49c7-89ad-5c07de26c880"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.7231 | Val Loss: 0.7332 | Accuracy: 0.4850 | Precision: 0.4849 | Recall: 1.0000\n",
            "Epoch 02 | Training Loss: 0.7201 | Val Loss: 0.7300 | Accuracy: 0.4850 | Precision: 0.4849 | Recall: 1.0000\n",
            "Epoch 03 | Training Loss: 0.7175 | Val Loss: 0.7270 | Accuracy: 0.4850 | Precision: 0.4849 | Recall: 1.0000\n",
            "Epoch 04 | Training Loss: 0.7151 | Val Loss: 0.7243 | Accuracy: 0.4852 | Precision: 0.4850 | Recall: 1.0000\n",
            "Epoch 05 | Training Loss: 0.7128 | Val Loss: 0.7217 | Accuracy: 0.4854 | Precision: 0.4851 | Recall: 1.0000\n",
            "Epoch 06 | Training Loss: 0.7108 | Val Loss: 0.7194 | Accuracy: 0.4852 | Precision: 0.4850 | Recall: 0.9996\n",
            "Epoch 07 | Training Loss: 0.7089 | Val Loss: 0.7173 | Accuracy: 0.4848 | Precision: 0.4848 | Recall: 0.9979\n",
            "Epoch 08 | Training Loss: 0.7072 | Val Loss: 0.7153 | Accuracy: 0.4848 | Precision: 0.4848 | Recall: 0.9967\n",
            "Epoch 09 | Training Loss: 0.7057 | Val Loss: 0.7135 | Accuracy: 0.4848 | Precision: 0.4847 | Recall: 0.9950\n",
            "Epoch 10 | Training Loss: 0.7042 | Val Loss: 0.7117 | Accuracy: 0.4862 | Precision: 0.4854 | Recall: 0.9942\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.7054 | Test Accuracy: 0.5022 | Test Precision: 0.5011 | Test Recall: 0.9961\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function tanh with an optimizer ADAM , batach size=512\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.tanh(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.tanh(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ca151lFurE4",
        "outputId": "b8cdc583-cc98-4650-95af-befd2886d855"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6760 | Val Loss: 0.6713 | Accuracy: 0.5784 | Precision: 0.6129 | Recall: 0.3540\n",
            "Epoch 02 | Training Loss: 0.6690 | Val Loss: 0.6632 | Accuracy: 0.6096 | Precision: 0.5866 | Recall: 0.6597\n",
            "Epoch 03 | Training Loss: 0.6620 | Val Loss: 0.6623 | Accuracy: 0.6080 | Precision: 0.5812 | Recall: 0.6848\n",
            "Epoch 04 | Training Loss: 0.6604 | Val Loss: 0.6613 | Accuracy: 0.6064 | Precision: 0.5913 | Recall: 0.6093\n",
            "Epoch 05 | Training Loss: 0.6589 | Val Loss: 0.6610 | Accuracy: 0.6082 | Precision: 0.5919 | Recall: 0.6180\n",
            "Epoch 06 | Training Loss: 0.6626 | Val Loss: 0.6669 | Accuracy: 0.6008 | Precision: 0.5659 | Recall: 0.7583\n",
            "Epoch 07 | Training Loss: 0.6588 | Val Loss: 0.6619 | Accuracy: 0.6054 | Precision: 0.5798 | Recall: 0.6757\n",
            "Epoch 08 | Training Loss: 0.6575 | Val Loss: 0.6607 | Accuracy: 0.6076 | Precision: 0.5847 | Recall: 0.6576\n",
            "Epoch 09 | Training Loss: 0.6560 | Val Loss: 0.6606 | Accuracy: 0.6076 | Precision: 0.5974 | Recall: 0.5846\n",
            "Epoch 10 | Training Loss: 0.6565 | Val Loss: 0.6619 | Accuracy: 0.6066 | Precision: 0.5782 | Recall: 0.6972\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6595 | Test Accuracy: 0.6068 | Test Precision: 0.5901 | Test Recall: 0.6994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function leakyrelu with an optimizer ADAM , batach size=512\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvfcitUBvooc",
        "outputId": "0f2c80db-1a06-466a-90bf-c9d75cd316ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6781 | Val Loss: 0.6684 | Accuracy: 0.5922 | Precision: 0.6061 | Recall: 0.4538\n",
            "Epoch 02 | Training Loss: 0.6665 | Val Loss: 0.6639 | Accuracy: 0.6066 | Precision: 0.5944 | Recall: 0.5936\n",
            "Epoch 03 | Training Loss: 0.6596 | Val Loss: 0.6617 | Accuracy: 0.6126 | Precision: 0.5885 | Recall: 0.6679\n",
            "Epoch 04 | Training Loss: 0.6582 | Val Loss: 0.6609 | Accuracy: 0.6138 | Precision: 0.5899 | Recall: 0.6671\n",
            "Epoch 05 | Training Loss: 0.6559 | Val Loss: 0.6603 | Accuracy: 0.6114 | Precision: 0.5916 | Recall: 0.6407\n",
            "Epoch 06 | Training Loss: 0.6587 | Val Loss: 0.6609 | Accuracy: 0.6066 | Precision: 0.5793 | Recall: 0.6885\n",
            "Epoch 07 | Training Loss: 0.6538 | Val Loss: 0.6605 | Accuracy: 0.6086 | Precision: 0.5818 | Recall: 0.6848\n",
            "Epoch 08 | Training Loss: 0.6529 | Val Loss: 0.6596 | Accuracy: 0.6114 | Precision: 0.5894 | Recall: 0.6543\n",
            "Epoch 09 | Training Loss: 0.6509 | Val Loss: 0.6592 | Accuracy: 0.6094 | Precision: 0.5940 | Recall: 0.6139\n",
            "Epoch 10 | Training Loss: 0.6508 | Val Loss: 0.6596 | Accuracy: 0.6090 | Precision: 0.5831 | Recall: 0.6790\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6579 | Test Accuracy: 0.6101 | Test Precision: 0.5960 | Test Recall: 0.6835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function leakyrelu with an optimizer SGD , batach size=512\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jCKCYpUwFyK",
        "outputId": "a11be725-1f96-4fda-ce0e-95a47e20c7ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.7111 | Val Loss: 0.7081 | Accuracy: 0.4954 | Precision: 0.4895 | Recall: 0.9563\n",
            "Epoch 02 | Training Loss: 0.6968 | Val Loss: 0.6983 | Accuracy: 0.5114 | Precision: 0.4977 | Recall: 0.8461\n",
            "Epoch 03 | Training Loss: 0.6921 | Val Loss: 0.6939 | Accuracy: 0.5242 | Precision: 0.5064 | Recall: 0.7318\n",
            "Epoch 04 | Training Loss: 0.6902 | Val Loss: 0.6920 | Accuracy: 0.5282 | Precision: 0.5108 | Recall: 0.6345\n",
            "Epoch 05 | Training Loss: 0.6895 | Val Loss: 0.6910 | Accuracy: 0.5332 | Precision: 0.5164 | Recall: 0.5862\n",
            "Epoch 06 | Training Loss: 0.6891 | Val Loss: 0.6906 | Accuracy: 0.5336 | Precision: 0.5173 | Recall: 0.5668\n",
            "Epoch 07 | Training Loss: 0.6888 | Val Loss: 0.6902 | Accuracy: 0.5368 | Precision: 0.5206 | Recall: 0.5631\n",
            "Epoch 08 | Training Loss: 0.6886 | Val Loss: 0.6900 | Accuracy: 0.5364 | Precision: 0.5204 | Recall: 0.5569\n",
            "Epoch 09 | Training Loss: 0.6884 | Val Loss: 0.6896 | Accuracy: 0.5354 | Precision: 0.5200 | Recall: 0.5425\n",
            "Epoch 10 | Training Loss: 0.6881 | Val Loss: 0.6893 | Accuracy: 0.5380 | Precision: 0.5233 | Recall: 0.5276\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6891 | Test Accuracy: 0.5390 | Test Precision: 0.5397 | Test Recall: 0.5295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function leakyrelu with an optimizer RMSprop , batach size=512\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 512\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwTxx2G6xDNA",
        "outputId": "6f019692-5897-4d6e-f048-9e9b71e09160"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6854 | Val Loss: 0.7022 | Accuracy: 0.5282 | Precision: 0.6585 | Recall: 0.0557\n",
            "Epoch 02 | Training Loss: 0.6683 | Val Loss: 0.6655 | Accuracy: 0.6054 | Precision: 0.5962 | Recall: 0.5767\n",
            "Epoch 03 | Training Loss: 0.6643 | Val Loss: 0.6639 | Accuracy: 0.6042 | Precision: 0.5737 | Recall: 0.7145\n",
            "Epoch 04 | Training Loss: 0.6621 | Val Loss: 0.6651 | Accuracy: 0.6048 | Precision: 0.6197 | Recall: 0.4785\n",
            "Epoch 05 | Training Loss: 0.6611 | Val Loss: 0.6705 | Accuracy: 0.5832 | Precision: 0.6296 | Recall: 0.3408\n",
            "Epoch 06 | Training Loss: 0.6612 | Val Loss: 0.6732 | Accuracy: 0.5792 | Precision: 0.5445 | Recall: 0.8082\n",
            "Epoch 07 | Training Loss: 0.6601 | Val Loss: 0.6764 | Accuracy: 0.5768 | Precision: 0.5419 | Recall: 0.8218\n",
            "Epoch 08 | Training Loss: 0.6579 | Val Loss: 0.6718 | Accuracy: 0.5876 | Precision: 0.5520 | Recall: 0.7933\n",
            "Epoch 09 | Training Loss: 0.6584 | Val Loss: 0.6608 | Accuracy: 0.6102 | Precision: 0.6022 | Recall: 0.5771\n",
            "Epoch 10 | Training Loss: 0.6552 | Val Loss: 0.6802 | Accuracy: 0.5780 | Precision: 0.6498 | Recall: 0.2809\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6841 | Test Accuracy: 0.5690 | Test Precision: 0.6653 | Test Recall: 0.2779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function leakyrelu with an optimizer ADAM , batach size=256\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMazspEOxqqV",
        "outputId": "d10adb9c-dc8a-4449-c370-52a13b9e4adf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6738 | Val Loss: 0.6652 | Accuracy: 0.6022 | Precision: 0.5959 | Recall: 0.5573\n",
            "Epoch 02 | Training Loss: 0.6623 | Val Loss: 0.6667 | Accuracy: 0.5962 | Precision: 0.5620 | Recall: 0.7574\n",
            "Epoch 03 | Training Loss: 0.6602 | Val Loss: 0.6612 | Accuracy: 0.6070 | Precision: 0.5845 | Recall: 0.6547\n",
            "Epoch 04 | Training Loss: 0.6591 | Val Loss: 0.6618 | Accuracy: 0.6074 | Precision: 0.5771 | Recall: 0.7116\n",
            "Epoch 05 | Training Loss: 0.6560 | Val Loss: 0.6609 | Accuracy: 0.6096 | Precision: 0.6067 | Recall: 0.5536\n",
            "Epoch 06 | Training Loss: 0.6535 | Val Loss: 0.6627 | Accuracy: 0.6096 | Precision: 0.5818 | Recall: 0.6922\n",
            "Epoch 07 | Training Loss: 0.6531 | Val Loss: 0.6598 | Accuracy: 0.6090 | Precision: 0.5959 | Recall: 0.6011\n",
            "Epoch 08 | Training Loss: 0.6505 | Val Loss: 0.6577 | Accuracy: 0.6094 | Precision: 0.5905 | Recall: 0.6337\n",
            "Epoch 09 | Training Loss: 0.6486 | Val Loss: 0.6581 | Accuracy: 0.6128 | Precision: 0.5985 | Recall: 0.6118\n",
            "Epoch 10 | Training Loss: 0.6468 | Val Loss: 0.6636 | Accuracy: 0.6038 | Precision: 0.5710 | Recall: 0.7347\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6594 | Test Accuracy: 0.6104 | Test Precision: 0.5877 | Test Recall: 0.7402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 655\n",
        "#Learning Rate of 0.001 and actiation function leakyrelu with an optimizer ADAM , batach size=128\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(655)\n",
        "np.random.seed(655)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLG95egcyvMP",
        "outputId": "058a5ced-a259-41de-b5b9-38faf28fca91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6717 | Val Loss: 0.6658 | Accuracy: 0.6056 | Precision: 0.5763 | Recall: 0.7042\n",
            "Epoch 02 | Training Loss: 0.6643 | Val Loss: 0.6622 | Accuracy: 0.6092 | Precision: 0.5826 | Recall: 0.6836\n",
            "Epoch 03 | Training Loss: 0.6592 | Val Loss: 0.6641 | Accuracy: 0.6030 | Precision: 0.5946 | Recall: 0.5693\n",
            "Epoch 04 | Training Loss: 0.6580 | Val Loss: 0.6600 | Accuracy: 0.6096 | Precision: 0.5843 | Recall: 0.6745\n",
            "Epoch 05 | Training Loss: 0.6551 | Val Loss: 0.6652 | Accuracy: 0.5944 | Precision: 0.5601 | Recall: 0.7607\n",
            "Epoch 06 | Training Loss: 0.6532 | Val Loss: 0.6596 | Accuracy: 0.6088 | Precision: 0.5809 | Recall: 0.6935\n",
            "Epoch 07 | Training Loss: 0.6503 | Val Loss: 0.6598 | Accuracy: 0.6132 | Precision: 0.6001 | Recall: 0.6060\n",
            "Epoch 08 | Training Loss: 0.6480 | Val Loss: 0.6604 | Accuracy: 0.6064 | Precision: 0.6265 | Recall: 0.4658\n",
            "Epoch 09 | Training Loss: 0.6465 | Val Loss: 0.6589 | Accuracy: 0.6070 | Precision: 0.6013 | Recall: 0.5619\n",
            "Epoch 10 | Training Loss: 0.6449 | Val Loss: 0.6660 | Accuracy: 0.5968 | Precision: 0.5627 | Recall: 0.7554\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6611 | Test Accuracy: 0.6064 | Test Precision: 0.5806 | Test Recall: 0.7660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with seed number  of 1567\n",
        "#Learning Rate of 0.001 and actiation function leakyrelu with an optimizer ADAM , batach size=256\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(1567)\n",
        "np.random.seed(1567)\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (not used in compute_output here)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used in the forward pass)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # List of variables to update during backpropagation\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to compute the output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        # Cast X to float32\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.leaky_relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.leaky_relu(h2)\n",
        "        # Output layer (logits)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n"
      ],
      "metadata": {
        "id": "4u6vGBIAzcux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmlxiHTH7PM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}